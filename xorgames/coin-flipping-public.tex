%=======================

In this \Section, 
we treat the XOR target games where 
the size of the option sets are likely to be small. 
Looking at the big picture, 
we will devise leader election schemes in beacon protocols 
whose dynamic can be mapped to this XOR target game.
Then, if we can prove that the grinding power of this game is small, 
we can prove that the resulting beacons have high min-entropy as well.



\Paragraph{Leader election schemes and the size of option sets.}
Let $\Distribution$ be an arbitrary distribution on $\NN$ 
and let $S \sim \Distribution$.
Consider a uniform XOR target game where 
the sizes of its option sets are distributed as $1 + S$. 
In Lemma~\ref{lemma:xorgame-moment}, 
we derived a moment bound for the grinding power of this game using moment bounds on $1 + S$. 
In this \Section, we treat two specific incarnations of $\Distribution$, 
namely, the Poisson binomial distribution and 
the Bernoulli distribution.

The rationale for the Poisson binomial distribution (Definition~\ref{def:poisson-binomial-distribution}) 
for $S$ 
comes from private leader elections (see Section~\ref{sec:leader-election-public-private});
these elections are necessary for thwarting an adaptive adversary.
Similarly, the rationale for a binomial distribution for $S$ stems from 
public leader elections; 
these elections suffice against a static adversary.

\Paragraph{The Poisson game captures the private leader election setting in eventual consensus PoS.}
Consider the set of dishonest players with total (dishonest) stake $\alpha$ 
participating 
in a private leader election scheme 
at a given slot. 
Let $w_\Adversary$ be the number of adversarial winners for this slot.
Suppose the beacon protocol chooses an arbitrary ``winning nonce'' for this slot from the set of all submitted nonces.  
(For example, in the protocol in Definition~\ref{def:new-beacon}, this is the lexicographically smallest nonce.)
Out of the $w_\Adversary$ adversarial winners, 
suppose $c_\Adversary$ nonces are candidates for the winning nonce. 
The adversarial players can strategically emit nonces 
to force any of these $c_\Adversary$ nonces to become the winning nonce.
Thus, the size of the option set for this slot is $1 + c_\Adversary$; 
the ``plus one'' represents the choice for the adversary where 
he strategically allows an honest input to become the winning nonce. 

Notice that the distribution of $c_\Adversary$ is somewhat exotic 
whereas $w_\Adversary$ has the Poisson binomial distribution with parameter $\alpha$.
Moreover, $c_\Adversary$ is clearly upper bounded by $w_\Adversary$. 
Thus, we can derive moment bounds on $1 + c_\Adversary$ from 
the moment bounds on $1 + w_\Adversary$ 
and then arguing stochastic dominance.
The corresponding XOR target game, called the $(\ell, n, \alpha)$-Poisson game, 
is treated in Section~\ref{sec:poisson}. 
The resulting beacon (cf. Theorem~\ref{thm:beacon-poisson-multi-epoch}) 
is a superior replacement for the Praos beacon (cf. Theorem~\ref{thm:minentropy-loss-praos-multi-epochs}).


\Paragraph{The Bernoulli game captures the public leader election setting in eventual consensus PoS.}
The rationale for a binomial distribution for $S$ is similar.
When the leader election is a public one (see Section~\ref{sec:leader-election-public-private}), 
there is exactly one winner for a slot. 
If this winner is honest, he has just one option for his nonce: always submit it.
An adversarial winner, in contrast, has two options: 
either submit it soon, or not submit at all. 
It follows that the size of the option set for this slot is $1+S$ where 
$S \in \{0,1\}, \Pr[S = 1] = \alpha$ and $\Pr[S = 0] = 1 - \alpha$.
The corresponding XOR target game, called the $(\ell, n, \alpha)$-Poisson game, 
is treated in Section~\ref{sec:poisson}.
The resulting beacon (cf. Theorem~\ref{thm:beacon-bernoulli-multi-epoch}) 
can be an alternative to the Ouroboros classic beacon~\cite{Ouroboros}.


The goal of this section is to derive tail bounds on 
the grinding powers in the $(\ell,n,\alpha)$-Poisson game 
(Lemma~\ref{lemma:xor-game-poisson-gamma})
and the $(\ell, n, \alpha)$-Bernoulli game 
(Lemma~\ref{lemma:xor-game-Bernoulli-gamma}).






%

\section{The Poisson game}\label{sec:poisson}

  In this section, 
  we treat option sets whose size has the Poisson binomial distribution.


  \begin{definition}[Poisson binomial distribution]\label{def:poisson-binomial-distribution}
    Let $m \in \NN, \alpha \in (0, 1/2)$ and 
    $\sigma_i \in (0, 1), i \in[m]$ so that $\sum_{i=1}^m \sigma_i = \alpha$.
    Let $X_1, \ldots, X_m \in \{0,1\}$ be independent Bernoulli random variables with 
    $\Pr[X_i = 1] = \sigma_i$. 
    Define $S = \sum_{i = 1}^m X_i$ 
    and let $\PoissonBinomial(\alpha; \sigma_1, \ldots \sigma_m)$ be the distribution of $S$. 
    Then $\PoissonBinomial(\alpha)$ is called the \emph{Poisson binomial distribution} 
    with parameters $\alpha$ and $\sigma_1, \ldots, \sigma_m$. 
    
    When $\sigma_1, \sigma_2, \ldots$ or the number $m$ are not specified, 
    we consider the family of distributions where $\alpha$ is fixed but the breakdown 
    $\{\sigma_1, \sigma_2, \ldots, \}$ is arbitrary; 
    then $\PoissonBinomial(\alpha)$ denotes an arbitrary distribution 
    from this family.
  \end{definition}
  \noindent
  The name ``Poisson binomial'' has the following rationale. 
  Let $\Poisson(\alpha)$ be the Poisson distribution with parameter $\alpha$.
  That is, for $k \in \NN$, 
  $$
    \Pr_{T \sim \Poisson(\alpha)}[T = k] = \frac{\alpha^k k^\alpha}{k!}\,.
  $$
  LeCam's theorem (below) states that $\PoissonBinomial(\alpha)$ is ``statistically close'' to $\Poisson(\alpha)$.
  \begin{theorem*}[LeCam 1960]
  $$
    \sum_k \left\vert \Pr[S = k] - \frac{\alpha^k k^\alpha}{k!} \right\vert < \sum_{k=1}^n \sigma_k^2
  $$
  \end{theorem*}

  As we mentioned before, 
  we are interested in the grinding power in an $(\ell, n, \PoissonBinomial(\alpha))$-game, 
  or $(\ell, n, \alpha)$-Poisson game in short. 
  

  \begin{definition}[$(\ell, n, \alpha)$-Poisson game]\label{def:xor-game-poisson}
      Let $\ell, n \in \NN$ and $\alpha \in (0, 1/2)$.
      Then an $(\ell, n, \PoissonBinomial(\alpha) )$-game (see Definition~\ref{def:ell-n-dist-game})
      is called an 
      \emph{$(\ell, n, \alpha)$-Poisson game}.
  \end{definition}


  \begin{theorem}[Moment bound, Poisson game]\label{thm:xor-game-private-election}
      Let $\ell, n \in \NN, n \geq 2$ and $\alpha \in (0, 1/2)$. 
      Let
      \begin{equation}\label{eq:lambda-star-poisson}
          \lambda = \begin{cases}
              2 & \quad\text{if $\alpha \leq 0.41$\,,}\\
              \frac{2 \ln(1/\alpha)}{\ln(1+3\alpha+\alpha^2)} &\quad\text{otherwise\,.}
          \end{cases}
      \end{equation}
      Let $U$ be an $(\ell, n, \alpha)$-Poisson game with grinding power $g$. 
      Let $A$ be the event that 
      at least one of the option sets $P_{\ell + i}, i \in [n]$ in $U$ is $\{\star\}$; 
      condition on $A$. 
      Then 
      $$
          \Exp[g^\lambda \mid A] 
          \leq \frac{n^{1 + \lambda} }{1+\alpha} 
          \cdot \begin{cases}
              (1 + 3 \alpha + \alpha^2)^{\ell} &\quad\text{if $\alpha \leq 0.41$\,,} \\
              (1/\alpha)^\ell &\quad\text{otherwise\,.}
          \end{cases}
          \,.
      $$
      % \begin{align*}
      %     \Exp[g^2 \mid A] &\leq n^3 \cdot 
      %         (1 + 3 \alpha + \alpha^2)^\ell 
      %         &\qquad\text{if\quad $\alpha \in (0, 0.41]$, }\\
      %     \Exp[g^{1.37} \mid A] &\leq n^{2.37} \cdot 
      %         (1 + 3 \alpha + \alpha^2)^{0.685 \ell }
      %         &\qquad\text{if\quad $\alpha \in (0.41, 0.5)$}
      %         \,.
      % \end{align*}
  \end{theorem}
  \begin{proof}
    % First, note that 
    % $\ln(\Pr[S \geq 0]) 
    % = \ln\left(\prod_{i = 1}^m (1 - \sigma_i) \right) 
    % = \sum_{i = 1}^m \ln (1 - \sigma_i)$. 
    % Since the function $x \mapsto \ln(1 -x)$ is concave and super-additive for $x \in (0, 1)$, 
    % it follows that $\ln(\Pr[S \geq 0]) \geq \ln (1 - \sum_i \sigma_i) = \ln (1 - \alpha)$. 
    % Since logarithm is a monotone function, $\Pr[S \geq 0] \geq 1 - \alpha$ and, 
    % consequently, $$\Pr[S \geq 1] \leq \alpha\,.$$
    Recall the definition of an $(\ell, n, \alpha)$-Poisson game from Definition~\ref{def:xor-game-poisson}.
    and 
    the random variable $S$ therein.
    Specifically, let $m \in \NN$ and 
    let $\sigma_i \in (0, 1), i \in [m]$ so that $\sum_{i=1}^m \sigma_i = \alpha$.
    Let $X_1, \ldots, X_m \in \{0,1\}$ be independent Bernoulli random variables with 
    $\Pr[X_i = 1] = \sigma_i, i \in [m]$. 
    Let $S = \sum_i X_i$ 
    and note that $S \sim \PoissonBinomial(\alpha; \sigma_1, \ldots, \sigma_m)$.
    By Markov's inequality,
    $$
        \Pr[S \geq 1] \leq \frac{\Exp S}{1} = \alpha \,.
    $$

    \Paragraph{Case: $\alpha \in (0, 0.41]$.}
    Let $X'_i \in \{0,1\}, i \in [m]$ be i.i.d. Bernoulli random variables 
    with $\Pr[X'_i] = \alpha/m$. 
    Define $S' = \sum_{i=1}^m X'_i$ 
    so that $S'\sim \Binomial(m, \alpha/m)$. 
    Fact~\ref{fact:second-moment-equal-unequal-stake} states that 
    $\Exp S^2 \leq \Exp {S'}^2 \leq \alpha + \alpha^2$. 
    Since $\Exp S = \alpha$, it follows that 
    $$
    \Exp (1 + S)^2 
    \leq 1 + 2 \Exp S + \Exp S^2
    \leq 1 + 3 \alpha + \alpha^2
    \,.
    $$
    Since $\alpha < 1/2$, the quantity 
    $\Pr[S \geq 1] \cdot \Exp (1 + S)^2$ is at most $\alpha\cdot (1 + 3.5 \alpha)$. 
    This quantity is convex and increasing in $\alpha$. 
    The unique solution to the quadratic equation $3.5 \alpha^2 + \alpha - 1 = 0$ 
    for $\alpha \in (0, 1/2)$ 
    is $\alpha = (-1 + \sqrt{1 + 14})/7 = 0.4104$. 
    Therefore, for any $\alpha \leq 0.41$, 
    the random variable $S$ satisfies~\eqref{eq:lambda-S-requirement} 
    with $\lambda = 2$.
    By Lemma~\ref{lemma:xorgame-moment}, we conclude that 
    $\Exp[g^2 \mid A] \leq n^3 (1 + 3 \alpha + \alpha^2)^\ell$ 
    for $\alpha \in (0, 0.41]$.
    
    \Paragraph{Case: $\alpha \in (0.41, 0.5)$.}
    In this case, we use Holder's inequality for any $\lambda \in (1,2)$ 
    and observe that 
    $$
        \Exp (1+S)^\lambda 
        \leq \left( \Exp (1+S)^2 \right)^{\lambda/2} 
        = (1 + 3\alpha +\alpha^2)^{\lambda/2}
        \,.
    $$
    Recalling~\eqref{eq:lambda-S-requirement}, 
    we want to find the maximal $\lambda \in (1, 2)$ 
    which satisfies $\alpha(1 + 3\alpha +\alpha^2)^{\lambda/2} \leq 1$ or, 
    equivalently, setting $\lambda$ as in~\eqref{eq:lambda-star-poisson}. 
    Note that this implies $\Exp (1+S)^\lambda \leq 1 + 3\alpha +\alpha^2)^{\lambda/2} \leq 1/\alpha$.
    The claim follows from applying Lemma~\ref{lemma:xorgame-moment} 
    with $\lambda$ from~\eqref{eq:lambda-star-poisson}. 
  \end{proof}
  % \hfill\qed


  \begin{lemma}[Tail bound, Poisson game]\label{lemma:xor-game-poisson-gamma}
      Let $\EpsP \in (0, 1), \alpha \in (0, 1/2), \ell, n \in \NN$ and $n \geq \ell$. 
      Let $\lambda$ be defined in~\eqref{eq:lambda-star-poisson}.
      Let 
      \begin{equation}
          \label{eq:xor-poisson-tail-gamma}
          % \gamma = (1 + 3 \alpha + \alpha^2)^{\ell/2} \cdot \begin{dcases}            
          %     n^{1.50} \EpsP^{-0.50} \,,&\quad\text{if}\quad\alpha \in (0, 0.41]\,, \\
          %     n^{1.73} \EpsP^{-0.73} \,,  &\quad\text{if}\quad\alpha \in (0.41, 0.50)\,.
          % \end{dcases}
          \gamma = \left(\frac{n^{1 + \lambda} }{ (1+\alpha) \EpsP} \right)^{1/\lambda}
              \cdot \begin{cases}
                  (1 + 3 \alpha + \alpha^2)^{\ell/\lambda} &\quad\text{if $\alpha \leq 0.41$\,,} \\
                  (1/\alpha)^{\ell/\lambda} &\quad\text{otherwise\,.}
              \end{cases}
      \end{equation}
      % and $\beta = 5/(12 - 14 \alpha)$.
      Let $g$ be the grinding power in an $(\ell, n, \alpha)$-Poisson game. 
      Then $\Pr[g \geq \gamma] \leq \alpha^n + \EpsP$.
  \end{lemma}
  \begin{proof}
      Let $g$ be the grinding power of an $(\ell, n, \alpha)$-Poisson game $U$.
      Let $A$ be the event that 
      at least one of the option sets $P_{\ell + i}, i \in [n]$ in $U$ is $\{\star\}$. 
      Recall the random variable $S$ from Definition~\ref{def:xor-game-poisson}. 
      Recall that in the proof of Theorem~\ref{thm:xor-game-private-election}, 
      we established that $\Pr[S \geq 1] \leq \alpha$. 
      It follows that $\Pr[\overline{A}] = (\Pr[S \geq 1])^n \leq \alpha^n$. 
      Thus, for any non-negative real $\gamma$, 
      we have 
      $
      \Pr[g \geq \gamma] 
      \leq 1 \cdot \Pr[\overline{A}] + \Pr[g \geq \gamma \mid A] \cdot 1
      = \alpha^n + \Pr[g \geq \gamma \mid A]
      $. 
      The rest of the proof focuses on finding a $\gamma$ so that 
      for a given $\EpsP$, 
      $\Pr[g \geq \gamma \mid A]$ is at most $\EpsP$. 

      The bound in~\eqref{eq:xor-poisson-tail-gamma} follows from 
      Theorem~\ref{thm:xor-game-private-election} and the following fact.

      \begin{fact}[Tail bound from moment bound]\label{fact:grinding-max}\label{fact:tail-bound-from-moment-bound}
          Let $\GrindingPower$ be an arbitrary non-negative random variable, 
          $\EpsP \in (0, 1)$ and 
          $\GrindingMax, \lambda$ be arbitrary positive reals. 
          If $
          % \begin{equation}\label{eq:grinding-max}
              \GrindingMax \geq \left( \Exp[g^\lambda]/\EpsP \right)^{1/\lambda}
          % \end{equation}
          $
          then $\Pr[\GrindingPower \geq \GrindingMax] \leq \EpsP$.    
      \end{fact}
      \begin{proof}
          We can use Markov's inequality to write 
          $
          % \begin{equation}\label{eq:grinding-markov}
              \Pr[\GrindingPower \geq \GrindingMax] \leq \Exp[\GrindingPower^\lambda]/\GrindingMax^\lambda
          $.
          % \end{equation}
          If we require $\Pr[\GrindingPower \geq \GrindingMax] \leq \EpsP$, 
          it suffices to require 
          $\Exp[\GrindingPower^\lambda]/\GrindingMax^\lambda \leq \EpsP$, or
          $\GrindingMax^\lambda \geq \Exp[\GrindingPower^\lambda]/\EpsP$. 
          Taking the $\lambda$th root gives the lower bound on $\gamma$.
      \end{proof}
      \noindent
      % (Note that Lemma~\ref{lemma:praos-tail-gamma} uses Fact~\ref{fact:grinding-max} 
      % with $\lambda = 2$.)

      This completes the proof of Lemma~\ref{lemma:xor-game-poisson-gamma}.
  \end{proof}



\section{The Bernoulli game}\label{sec:bernoulli}
  % \section{Option sets of size at most two\texorpdfstring{; the $(\ell, n, \alpha)$-Bernoulli game}{}}

    Let us consider a uniform XOR target game where an option set 
    contains at most two elements. 
    From Definition~\ref{def:xor-game-explicit}, 
    we conclude that in this case, 
    an option set is either $\{\star\}$ 
    or $\{0^\kappa, s\}$ where $s$ is uniform in $\{0,1\}^\kappa$.
    For $\alpha \in (0,1)$, 
    let $\Bernoulli(\alpha)$ denote the distribution of a Bernoulli random variable 
    $B \in \{0,1\}$ where $\Pr[B = 1] = \alpha$.


    \begin{definition}[$(\ell, n, \alpha)$-Bernoulli game]
        \label{def:static-game}\label{def:Bernoulli-game}
        Let $\alpha \in (0, 1/2)$. 
        An $(\ell, n, \Bernoulli(\alpha))$-game 
        is called the \emph{$(\ell, n, \alpha)$-Bernoulli game}.
    \end{definition}
    The following is an immediate corollary from Theorem~\ref{thm:xor-game-lookahead}.

    \begin{theorem}\label{thm:xor-game-playornot}
        Let $\alpha \in (0, 1/2)$ and $\lambda \in (0, \log_2(1/\alpha)]$. 
        Let $U$ be an $(\ell, n, \alpha)$-Bernoulli game. 
        Let $g$ be the grinding power of $U$. 
        Let $A$ be the event that 
        at least one of the option sets $P_{\ell + i}, i \in [n]$ is $\{\star\}$; 
        condition on $A$. 
        Then 
        $$
            \Exp [g^\lambda \mid A] \leq 2^{\lambda(\ell + 1)} n/(1+\alpha)
            \,.
        $$
    \end{theorem}

    % \begin{lemma}[Grinding power moments in the $(\ell, n, \alpha)$-Bernoulli game]
    % \label{lemma:xor-game-bernoulli-moment}
    % \end{lemma}
    \begin{proof}
        We retrace the proof of Lemma~\ref{lemma:xorgame-moment} 
        using $S = B$ and $r_i = 1 - B_i, i \in [\ell + n]$; 
        hence $|P_i| = 1 + B, i \in [\ell + n]$. 
        In particular, the random variables $H, r_i, i \in [\ell + n]$ 
        and the 
        events $A, A_h, h \in [n]$ remain the same. 
        % Let $H \defeq \max \{i : \text{$i \in [\ell+1, \ell+n]$ and $B_i = 0$} \}$. 
        % By our assumptions, $H \in [\ell+1, \ell+n]$ is well-defined, 
        % at least one of the $B_i$s is zero, and hence, 
        % the leading product in~\eqref{eq:xor-game-lookahead-gp} 
        % vanishes. 
        % Condition on the event $H = h$; 
        % this means $B_{\ell + h} = 0$ and 
        % $B_i = 1$ for $i \geq \ell + h+1$. 
        % Thus the additive terms for $k \leq h - 1$ 
        % in~\eqref{eq:xor-game-lookahead-gp}
        % vanishes as well.
        % Since $|P_i| \in \{1, 2\}$, 
        % the expression in~\eqref{eq:xor-game-lookahead-gp} becomes 
        Thus $\beta = 1 - \alpha$ and 
        $\Pr[H = h] = (1 - \alpha) \alpha^{n - h}$ for $h \in [n]$. 
        Condition on $A_h$. 
        Since $1 + S \leq 2$ and $|P_i| = 2$ for $i \geq \ell + h + 1$, 
        we can upper bound the right-hand side in~\eqref{eq:ell-n-d-game-bound-gh} as 
        \begin{align*}
        % \label{eq:xor-game-playornot-gp}
            g_h
            &\leq (1 + B)^\ell \sum_{k = h}^n 2^{n - k}
            % \sum_{k=h}^n (1 - B_k) \, 
            %     \left(  \prod_{i=k}^{\ell+k-1}      (1 + B_i)   \right)   
            %     \left(  \prod_{i=\ell+k+1}^{\ell+n} 2 B_i       \right)    
            %     \\        
            % &\leq 
            %     \sum_{k=h}^{n }
            %     (1 - B_k)\,
            %     2^\ell\,
            %     2^{n - k}            
            \leq 
                2^{\ell} \cdot ( 1 + 2 + \cdots + 2^{n - h} ) 
            = 2^{n + \ell - h + 1}
        \,.
        \end{align*}
        It follows that
        $
            \Exp [g_h^\lambda \mid A_h]
            \leq 
                2^{\lambda(n + \ell - h + 1)} 
        $ 
        since 
        the quantities $|P_{i}|$ in the game $U$ are independent. 
        Since $\Pr[A] = 1 - \alpha^n$, we have 
        \begin{align*}
            \Exp [g^\lambda \mid A]
                &=
                \sum_{h = 1}^n \Pr[A_h \mid A] 
                    \Exp [g_h^\lambda \mid A_h] 
                =
                \sum_{h = 1}^n 
                    \frac{(1 - \alpha) \alpha^{n-h}}{1 - \alpha^n}\,
                    \Exp [g_h^\lambda \mid A_h] 
                \leq 
                    \frac{2^{\lambda(\ell + 1)}}{1+\alpha}
                    \sum_{h=1}^n 
                    \left( \alpha 2^\lambda \right)^{n-h} 
        \end{align*}
        since $(1 - \alpha)/(1 - \alpha^n) \leq 1/(1 + \alpha)$ for any $n \geq 2$. 
        By assumption, $\lambda \leq \log_2(1/\alpha)$ or, 
        equivalently, $\alpha 2^\lambda \leq 1$.
        It follows that 
        $
            \Exp [g^\lambda \mid A] \leq  n \cdot 2^{\lambda(\ell + 1)}/(1+\alpha)
        $.
    \end{proof}


    \begin{lemma}\label{lemma:xor-game-Bernoulli-gamma}
        Let $\EpsP \in (0,1), \ell, n \in \NN, n \geq 2$ and 
        $\alpha \in (0, 1/2)$.
        Let $U$ be a $(\ell, n, \alpha)$-Bernoulli game. 
        The grinding power $g = g(U)$ 
        satisfies $\Pr[g \geq \gamma] \leq \alpha^n + \EpsP$ where 
        \begin{align}\label{eq:bernoulli-game-gamma}
            % {\gamma = n 2^{\ell+1 + q/\log_2(1/\alpha)}}
            \gamma = 2^{\ell + 1} \left(
                \frac{n}{(1+\alpha)\EpsP}
            \right)^{1/\log_2(1/\alpha)}
            % \quad\text{and}\quad q = \log_2(1/\EpsP)
            \,.
        \end{align}
    \end{lemma}
    \begin{proof}
        Let $\gamma$ be a positive integer whose value will be set later.
        Let $A$ be the event that 
        at least one of the option sets $P_{\ell + i}, i \in [n]$ in $U$ is $\{\star\}$. 
        (Note that $\Pr[\overline{A}] = \alpha^n$.)
        Theorem~\ref{thm:xor-game-playornot} states that 
        for any $\lambda \in (0, \log_2(1/\alpha)]$, 
        $\Exp [g^\lambda \mid A]$ is at most $n 2^{\lambda(\ell + 1)}$. 
        Since $g$ is non-negative, 
        Markov's inequality implies that 
        $$
        \Pr[g \geq \gamma] 
        \leq \Pr[\overline{A}] + \Pr[g \geq \gamma | A]
        \leq \alpha^n  + \Exp[g^\lambda \mid A]/\gamma^\lambda
        \leq \alpha^n  + \frac{n}{1+\alpha} 
            \left(\frac{2^{\ell + 1}}{\gamma}\right)^\lambda
        \,.$$ 
        Writing $a = 2^{\ell + 1}/\gamma$, 
        the derivative of this upper bound with respect to $\lambda$ is 
        $n a^\lambda \ln(a) / (1+\alpha)$ . 
        This quantity is non-positive as long as $a \leq 1$ or, equivalently, 
        $\gamma \geq 2^{\ell + 1}$. 
        Thus we get the tightest upper bound on $\Pr[g \geq \gamma]$ 
        by taking the largest permitted value of $\lambda$ 
        as long as $\gamma$ is at least $2^{\ell + 1}$. 
        
        Set $\lambda = \log_2(1/\alpha)$ and take any $\EpsP \in (0,1)$. 
        For any real positive $x$, 
        the inequality $(n/(1+\alpha)) (2^{\ell + 1}/x)^\lambda \leq \EpsP$ 
        is equivalent to saying that 
        $x \geq 2^{\ell + 1} (n/(1+\alpha)\EpsP)^{1/\log_2(1/\alpha)}$. 
        Let us take 
        $\gamma \defeq 2^{\ell + 1} (n/(1+\alpha)\EpsP)^{1/\log_2(1/\alpha)}$ 
        so that this inequality is satisfied by setting $x = \gamma$. 
        This implies two things. 
        First, 
        since $\alpha > 0, \EpsP < 1$, and $n \geq 2$, 
        the quantity  $(n/(1+\alpha)\EpsP)^{1/\log_2(1/\alpha)}$ is at least one and, consequently, 
        $\gamma$ is clearly larger than $2^{\ell + 1}$. 
        Second, 
        the quantity 
        $\left( \Exp[g^\lambda | A]/\EpsP \right)^{1/\lambda}$ is at most $\gamma$ 
        and, using Fact~\ref{fact:grinding-max}, 
        $\Pr[g \geq \gamma | A]$ is at most $\EpsP$. 
        It follows that $\Pr[g \geq \gamma] \leq \alpha^n + \EpsP$.
    \end{proof}

