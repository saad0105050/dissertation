With the recursive formulation for relative margin in hand, 
we study the stochastic process that arises when the
characteristic string $w$ is chosen from a distribution 
satisfying the $\epsilon$-martingale condition. 
Let us write $w = xy$ (where the decomposition is arbitrary) and 
let $E$ be the event that the relative margin $\mu_x(y)$ is non-negative. 
As Fact~\ref{fact:margin-balance} and Observation~\ref{obs:settlement-balanced-fork} point out, 
this event has a direct bearing on the settlement violation on $w$. 

In this section, we prove two bounds on the probability of the event $E$.
The first bound corresponds to the distribution 
$\mathcal{B}_\epsilon$ 
whereas the second bound applies to any distribution that 
satisfies the $\epsilon$-martingale condition. 
(Recall that the distribution $\mathcal{B}_\epsilon$, mentioned in Theorem~\ref{thm:main}, 
satisfies the $\epsilon$-martingale condition with equality.)
Our exposition in this section culminates in the proofs of our main theorems. 

We start with the following theorem 
which is a direct consequences of these bounds; see Section~\ref{sec:bounds} for a proof.
\begin{theorem}\label{thm:plain-main}
  Let $T, k \in \NN$.
  Let $w \in \{0,1\}^T$ be a random variable
  satisfying the $\epsilon$-martingale condition. 
  Consider the decomposition $w = xy, |y| = k$.  
  Then
  \[
    \Pr_{w = xy}[\text{there is an $x$-balanced fork for $xy$}] 
    = \Pr_{w = xy}[\mu_x(y) \geq 0] 
    \leq \exp(-\Omega(k))
    % = \exp({-\epsilon^3 (1 - O(\epsilon)) k/2})
    \,.
  \]
  (The asymptotic notation hides constants that depend only on $\epsilon$.)
\end{theorem}
Notice how the final bound does not depend on $|x|$. 
Indeed, as we show in Lemma~\ref{lemma:rho-stationary}, 
the reach of a Boolean string $x$ 
drawn from the distribution $\mathcal{B}_\epsilon$ 
% mentioned in Theorem~\ref{thm:main} 
converges to a fixed exponential distribution as
$|x| \rightarrow \infty$. 
This limiting distribution ``stochastically dominates'' 
any distribution that satisfies the $\epsilon$-martingale condition; 
see Section~\ref{sec:dominance-rho-stationary}.
The following corollary is immediate.
% An appeal to Fact~\ref{fact:margin-balance} yields the following corollary.
\begin{corollary}\label{cor:main} 
  Let $T, s, k \in \NN$.
  Let $w \in \{0,1\}^T$ be a 
  random variable satisfying the $\epsilon$-martingale condition. 
  Then
  \begin{align}\label{eq:cor-main}
    \Pr_w\left[\parbox{65mm}{
      there is a decomposition $w = xyz$, 
      where $|x| = s - 1$ and $|y| \geq k$, 
      so that $\mu_x(y) \geq 0$ 
    }\right] 
      \leq O(1) \cdot \exp(-\Omega(k))
    \,.
  \end{align}
\end{corollary}
\begin{proof}
  Notice that Theorem~\ref{thm:plain-main} works for \emph{any} prefix $x$ 
  of the characteristic string $w = xy$.
  Thus we can fix the prefix $x$ with length $s - 1$ and 
  sum the bound in Theorem~\ref{thm:plain-main} 
  over all suffixes $y$ with length at least $k$. 
  This would give an upper bound to the left-hand side of our claim, 
  the bound being 
  $\sum_{t \geq k} \exp(-\Omega(t)) = O(1)\cdot \exp(-\Omega(k))$. 
\end{proof}

We obtain another imporant corollary by setting $|x| = 0$ and $|y| = n$ in Theorem~\ref{thm:plain-main}. 
\begin{corollary}\label{coro:forkable-rare}%[cf. \cite{KRDO17}]
  Let $w \in \{0,1\}^n$ be a random variable satisfying the $\epsilon$-martingale condition. Then
  \[
    \Pr[\text{$w$ is forkable}] = \Pr[\mu(w) \geq 0] \leq \exp(-\Omega(n))
    \,.
  \]
\end{corollary}
Thus \emph{forkable strings are rare} 
where ``forkable'' is defined in Definition~\ref{def:forkable}.
This result 
significantly strengthens the $\exp(-\Omega(\sqrt{n}))$ 
bound obtained in Theorem 4.13 of~\cite{KRDO17}. 
The improvement comes in two respects: 
first, Corollary~\ref{cor:main} improves the exponent from $\sqrt{n}$ to $n$, 
and second, the characteristic string is allowed to be drawn 
from any distribution satisfying the $\epsilon$-martingale condition. 
For comparison, the characteristic string in Theorem 4.13 of~\cite{KRDO17} 
has the distribution $\mathcal{B}_\epsilon$, i.e., 
the bits were i.i.d.\ Bernoulli random variables 
with expectation $(1 - \epsilon)/2$.




\section{Two bounds for non-negative relative margin}\label{sec:bounds}
The main ingredients to proving Theorem~\ref{thm:plain-main} 
are two bounds on the event that for a characteristic string $xy$, 
the relative margin $\mu_x(y)$ is non-negative. 

\begin{bound}\label{bound:analytic}
  Let $x \in \{0,1\}^m$ and $y \in \{0,1\}^k$ be independent random
  variables, each chosen according to $\mathcal{B}_\epsilon$. Then
  \[
    \Pr[\mu_x(y) \geq 0] 
      \leq \exp({-\epsilon^3 (1 - O(\epsilon)) k/2})
    \,.
  \]
\end{bound}
% We are also interested in characteristic
% strings drawn from a distribution $\mathcal{W}$ 
% which satisfies $\epsilon$-martingale condition. 
% There are settings, 
% such as Genesis~\cite{DBLP:journals/iacr/BadertscherGKRZ18}, 
% where this flexibility is important.  


\begin{bound}\label{bound:geometric}
  Let $x \in \{0,1\}^m$ and $y \in \{0,1\}^k$ be random variables
  (jointly) satisfying the $\epsilon$-martingale condition with
  respect to the ordering $x_1, \ldots, x_m, y_1, \ldots, y_k$.  Let
  $x^\prime \in \{0,1\}^m$ and $y^\prime \in \{0,1\}^k$ be independent
  random variables, each chosen independently according to
  $\mathcal{B}_\epsilon$.  Then
  \[
    \Pr[\mu_x(y) \geq 0] \leq \Pr[\mu_{x^\prime}(y^\prime) \geq 0]
%    \,.
%  \]
%  As a result, 
%  \[
%    \Pr[\mu_x(y) \geq 0] 
      \leq \exp({-\epsilon^3 (1 - O(\epsilon)) k/2})
    \,.
  \]
\end{bound}

\Paragraph{Proof of Theorem~\ref{thm:plain-main}.}
The equality is Fact~\ref{fact:margin-balance} 
and the inequality is Bound~\ref{bound:geometric}. $\qed$


% \hfill $\qed$ 



% \subsection{$\mathcal{B}$ stochastically dominates $\Distribution$; stationary distribution for reach}
\section{A stochastically dominant prefix distribution}\label{sec:dominance-rho-stationary}
\input{consistency/prefix-distribution} 

%=======================================================
%=======================================================
\section{Proof of Bound~\ref{bound:analytic}}\label{sec:gf-proof}

%\begin{proof}[of Bound~\ref{bound:analytic}]
% \begin{proof}
  Anticipating the proof, we make a few remarks about generating
  functions and stochastic dominance.  We reserve the term
  \emph{generating function} to refer to an ``ordinary'' generating
  function which represents a sequence $a_0, a_1, \ldots$ of
  non-negative real numbers by the formal power series
  $\gf{A}(Z) = \sum_{t = 0}^\infty a_t Z^t$. When
  $\gf{A}(1) = \sum_t a_t = 1$ we say that the generating function is
  a \emph{probability generating function}; in this case, the
  generating function $\gf{A}$ can naturally be associated with the
  integer-valued random variable $A$ for which $\Pr[A = k] = a_k$. If
  the probability generating functions $\gf{A}$ and $\gf{B}$ are
  associated with the random variables $A$ and $B$, it is easy to
  check that $\gf{A} \cdot \gf{B}$ is the generating function
  associated with the convolution $A + B$ (where $A$ and $B$ are
  assumed to be independent).  Translating the notion of stochastic
  dominance to the setting with generating functions, we say that the
  generating function $\gf{A}$ \emph{stochastically dominates}
  $\gf{B}$ if $\sum_{t \leq T} a_t \leq \sum_{t \leq T} b_t$ for all
  $T \geq 0$; we write $\gf{B} \dominatedby \gf{A}$ to denote this state of
  affairs. If $\gf{B}_1 \dominatedby \gf{A}_1$ and
  $\gf{B}_2 \dominatedby \gf{A}_2$ then
  $\gf{B}_1 \cdot \gf{B}_2 \dominatedby \gf{A}_1 \cdot \gf{A}_2$ and
  $\alpha \gf{B}_1 + \beta \gf{B}_2 \dominatedby \alpha \gf{A}_1 + \beta
  \gf{A}_2$ (for any $\alpha, \beta \geq 0$).  Moreover, if
  $\gf{B} \dominatedby \gf{A}$ then it can be checked that
  $\gf{B}(\gf{C}) \dominatedby \gf{A}(\gf{C})$ for any probability
  generating function $\gf{C}(Z)$, where we write $\gf{A}(\gf{C})$ to
  denote the composition $\gf{A}(\gf{C}(Z))$.


  Finally, we remark that
  if $\gf{A}(Z)$ is a generating function which converges as a
  function of a complex $Z$ for $|Z| < R$ for some non-negative $R$, 
  $R$ is called the \emph{radius of convergence} of $\gf{A}$.  
  It follows from \citep[Theorem 2.19]{WilfGF} that 
  $\lim_{k \rightarrow \infty} {a_k}R^k = 0$ and $|a_k| = O(R^{-k})$. 
	In addition, if $\gf{A}$ is a probability generating function associated with the
  random variable $A$ then it follows that
  $\Pr[A \geq T] = O(R^{-T})$.
  
  We define $p = (1 - \epsilon)/2$ and $q = 1 - p$ and 
  as in the proof of Bound~\ref{bound:geometric},
  consider the independent $\{0,1\}$-valued random variables
  $w_1, w_2, \ldots$ where $\Pr[w_t = 1] = p$. We also define the
  associated $\{\pm1\}$-valued random variables $W_t =
  (-1)^{1+w_t}$.
	
  
	Although our actual interest is in the random variable $\mu_x(y)$ 
	from~\eqref{eq:mu-relative-recursive} on a characteristic string $w=xy$, 
  we begin by analyzing the case when $|x|=0$. 

  %\vspace{-2ex}
	\Paragraph{Case 1: $x$ is the empty string.}
  In this case, the random variable $\mu_x(y)$ is identical to $\mu(w)$ 
  from~\eqref{eq:mu-recursive} with $w = y$. 
	Our strategy is to study the probability generating
  function
  \[
    \gf{L}(Z) = \sum_{t = 0}^\infty \ell_t Z^t
  \]
  where $\ell_t = \Pr[\text{$t$ is the last time $\mu_t =
    0$}]$. Controlling the decay of the coefficients $\ell_t$ suffices
  to give a bound on the probability that $w_1\ldots w_k$ is forkable
  because
  \[
    \Pr[\text{$w_1 \ldots w_k$ is forkable}] \leq 1 - \sum_{t =
      0}^{k-1} \ell_t = \sum_{t = k}^\infty \ell_t\,.
  \]
  It seems challenging to give a closed-form algebraic expression for
  the generating function $\gf{L}$; our approach is to develop a
  closed-form expression for a probability generating function
  $\gf{\hat{L}} = \sum_t \hat{\ell}_t Z^t$ which stochastically
  dominates $\gf{L}$ and apply the analytic properties of this closed
  form to bound the partial sums $\sum_{t \geq k} \hat{\ell}_k$.
  Observe that if $\gf{L} \dominatedby \gf{\hat{L}}$ then the series
  $\gf{\hat{L}}$ gives rise to an upper bound on the probability that
  $w_1\ldots w_k$ is forkable as
  $\sum_{t=k}^\infty \ell_t \leq \sum_{t=k}^\infty \hat{\ell}_t$.

  The coupled random variables $\rho_t$ and $\mu_t$ are Markovian
  in the sense that values $(\rho_s, \mu_s)$ for $s \geq t$ are
  entirely determined by $(\rho_t, \mu_t)$ and the subsequent
  values $W_{t+1}, \ldots$ of the underlying variables $W_i$. We
  organize the sequence
  $(\rho_0, \mu_0), (\rho_1, \mu_1), \ldots$ into ``epochs''
  punctuated by those times $t$ for which $\rho_t = \mu_t =
  0$. With this in mind, we define $\gf{M}(Z) = \sum m_t Z^t$ to be
  the generating function for the first completion of such an epoch,
  corresponding to the least $t > 0$ for which
  $\rho_t = \mu_t = 0$. As we discuss below, $\gf{M}(Z)$ is not a
  probability generating function, but rather
  $\gf{M}(1) = 1 - \epsilon$. It follows that
  \begin{equation}\label{eq:L-def}
    \gf{L}(Z) = \epsilon(1 + \gf{M}(Z) + \gf{M}(Z)^2 + \cdots) =
    \frac{\epsilon}{1 - \gf{M}(Z)}\,.
  \end{equation}
  Below we develop an analytic expression for a generating function
  $\gf{\hat{M}}$ for which $\gf{M} \dominatedby \gf{\hat{M}}$ and define
  $\gf{\hat{L}} = \epsilon/(1 - \gf{\hat{M}}(Z))$. We then proceed as
  outlined above, noting that $\gf{L} \dominatedby \gf{\hat{L}}$ and using
  the asymptotics of $\gf{\hat{L}}$ to upper bound the probability
  that a string is forkable.

  In preparation for defining $\gf{\hat{M}}$, we set down two
  elementary generating functions for the ``descent'' and ``ascent''
  stopping times. Treating the random variables $W_1, \ldots$ as
  defining a (negatively) biased random walk, define $\gf{D}$ to be
  the generating function for the \emph{descent stopping time} of the
  walk; this is the first time the random walk, starting at 0, visits
  $-1$. The natural recursive formulation of the descent time yields a
  simple algebraic equation for the descent generating function,
  $\gf{D}(Z) = qZ + pZ \gf{D}(Z)^2$, and from this we may conclude
  \[
    \gf{D}(Z) = \frac{1 - \sqrt{1 - 4pqZ^2}}{2pZ}\,.
  \]
  We likewise consider the generating function $\gf{A}(Z)$ for the
  \emph{ascent stopping time}, associated with the first time the
  walk, starting at 0, visits 1: we have
  $\gf{A}(Z) = pZ + qZ \gf{A}(Z)^2$ and
  \[
    \gf{A}(Z) = \frac{1 - \sqrt{1 - 4pqZ^2}}{2qZ}\,.
  \]
  Note that while $\gf{D}$ is a probability generating function, the
  generating function $\gf{A}$ is not: according to the classical
  ``gambler's ruin'' analysis~\cite{Grinstead:1997ng}, the probability
  that a negatively-biased random walk starting at 0 ever rises to 1
  is exactly $p/q$; thus $\gf{A}(1) = p/q$.

  Returning to the generating function $\gf{M}$ above, we note that an
  epoch can have one of two ``shapes'': in the first case, the epoch
  is given by a walk for which $W_1 = 1$ followed by a descent (so
  that $\rho$ returns to zero); in the second case, the epoch is
  given by a walk for which $W_1 = -1$, followed by an ascent (so that
  $\mu$ returns to zero), followed by the eventual return of $\rho$
  to 0. Considering that when $\rho_t > 0$ it will return to zero
  in the future almost surely, it follows that the probability that
  such a biased random walk will complete an epoch is
  $p + q(p/q) = 2p = 1 - \epsilon$, as mentioned in the discussion
  of~\eqref{eq:L-def} above. One technical difficulty arising in a
  complete analysis of $\gf{M}$ concerns the second case discussed
  above: while the distribution of the smallest $t > 0$ for which
  $\mu_t = 0$ is proportional to $\gf{A}$ above, the distribution of
  the smallest subsequent time $t'$ for which $\rho_{t'} = 0$
  depends on the value $t$. More specifically, the distribution of the
  return time depends on the value of $\rho_t$. Considering that
  $\rho_t \leq t$, however, this conditional distribution (of the
  return time of $\rho$ to zero conditioned on $t$) is
  stochastically dominated by $\gf{D}^t$, the time to descend $t$
  steps. This yields the following generating function $\gf{\hat{M}}$
  which, as described, stochastically dominates $\gf{M}$:
  \[
    \gf{\hat{M}}(Z) = pZ\cdot \gf{D}(Z) + qZ \cdot \gf{D}(Z) \cdot
    \gf{A}(Z\cdot \gf{D}(Z))\,.
  \]
  
  It remains to establish a bound on the radius of convergence of
  $\gf{\hat{L}}$. Recall that if the radius of convergence of
  $\gf{\hat{L}}$ is $\exp(\delta)$ it follows that
  $\Pr[\text{$w_1 \ldots w_k$ is forkable}] = O(\exp(-\delta k))$. A
  sufficient condition for convergence of
  $\gf{\hat{L}}(z) = \epsilon/(1 - \gf{\hat{M}}(z))$ at $z$ is that
  that all generating functions appearing in the definition of
  $\gf{\hat{M}}$ converge at $z$ and that the resulting value
  $\gf{\hat{M}}(z) < 1$.
  
  The generating function $\gf{D}(z)$ (and $\gf{A}(z)$) converges when
  the discriminant $1 - 4pqz^2$ is positive; equivalently
  $|z| < 1/\sqrt{1 - \epsilon^2}$ or
  $|z| < 1 + \epsilon^2/2 + O(\epsilon^4)$. Considering
  $\gf{\hat{M}}$, it remains to determine when the second term,
  $qz D(z) \gf{A}(z \gf{D}(z))$, converges; this is likewise determined by
  positivity of the discriminant, which is to say that
  \[
    1 - (1 - \epsilon^2)\left(\frac{1 - \sqrt{1 - (1 - \epsilon^2)z^2}}{1 - \epsilon}\right)^2 > 0\,.
  \]
  Equivalently,
  \[
    |z| <  \sqrt{\frac{1}{1 + \epsilon}\left(\frac{2}{\sqrt{1 - \epsilon^2}} - \frac{1}{1+\epsilon}\right)} = 1 + \epsilon^3/2 + O(\epsilon^4) 
		\, .	
  \]
  Note that when the series $pz \cdot \gf{D}(z)$ converges, it
  converges to a value less than $1/2$; the same is true of
  $qz \cdot \gf{A}(z)$. It follows that for
  $|z| = 1 + \epsilon^3/2 + O(\epsilon^4)$, $|\gf{\hat{M}}(z)| < 1$
  and $\gf{\hat{L}}(z)$ converges, as desired. We conclude that
	\begin{align}
	  \Pr[\text{$w_1 \ldots w_k$ is forkable}] &= \exp(-\epsilon^3(1 + O(\epsilon))k/2)\,.
	\label{eq:prob_forkable_gf}
	\end{align}

          %\vspace{-2ex}
	\Paragraph{Case 2: $x$ is non-empty.}
        The relative margin before $y$ begins is $\mu_x(\varepsilon)$.
        Recalling that $\mu_x(\varepsilon) = \rho(x)$ and conditioning on the event that $\rho(x) = r$, 
    let us define the random variables $\left\{ \tilde{\mu}_t \right\}$ for $t = 0, 1, 2, \cdots$ as follows: $\tilde{\mu}_0 = \rho(x)$ and
    \[
      %\, , \qquad \text{and} \qquad
      \Pr[\tilde{\mu}_t = s]\, =\, \Pr[\mu_x(y) = s \mid \rho(x) = r \text{ and } |y| = t ]
      \, .
    \]
    If the $\tilde{\mu}$ random walk makes the $r$th descent at some time $t < n$, then $\tilde{\mu}_t = 0$ and the remainder of the walk is 
	identical to an $(k-t)$-step $\mu$ random walk which we have already analyzed. 
	Hence we investigate the probability generating function
	\[
			\gf{B}_r(Z) = \gf{D}(Z)^r \gf{L}(Z) \quad \text{with coefficients} \quad
      b^{(r)}_t := \Pr[t \text{ is the last time } \tilde{\mu}_t = 0 \mid \tilde{\mu}_0 = r]	
  \]
  where $t = 0, 1, 2, \cdots$. Our interest lies in the quantity 
    \[
      b_t 
      := \Pr[t \text{ is the last time } \tilde{\mu}_t = 0] 
      = \sum_{r\geq 0}{  b^{(r)}_t \DistRho_m(r) } 
      \,,
     \]
  where the \emph{reach distribution} 
  $\DistRho_m : \Z \rightarrow [0,1]$ 
  associated with the random variable $\rho(x), |x| = m$ is defined as 
  \begin{align}\label{eq:dist-rho}
    % \DistRho_m(r) &= \Pr_{W \sim \mathcal{D}}[\rho(W) = r \Given W \text{ has length } m]
    \DistRho_m(r) &= \Pr_{x \SuchThat |x| = m}[\rho(x) = r]
    \, .
  \end{align}
     % from~\eqref{eq:dist-rho}.
     Let $\gf{R}_m(Z)$ be the probability generating function
     for the distribution $\DistRho_m$. 
     Using Lemma~\ref{lemma:rho-stationary} and Definition~\ref{def:dominance}, we deduce that
     $\gf{R}_m \dominatedby \gf{R}_\infty$ for every $m \geq 0$ since 
    $\DistRho_m \dominatedby \StationaryRho$.
     In addition, it is easy to check from~\eqref{eq:stationary} that
     the probability generating function for $\StationaryRho$ is in fact
     $\gf{R}_\infty(Z) = (1-\beta)/(1-\beta Z)$ where $\beta := (1-\epsilon)/(1+\epsilon)$. 
    Thus the generating function corresponding to the
     probabilities $\{b_t\}_{t=0}^\infty$ is
	\begin{align}
		\gf{B}(Z) 
		&= \sum_{t=0}^\infty{b_t Z^t} = \sum_{r=0}^\infty{\DistRho_m(r) \sum_{t=0}^\infty{b_t^{(r)} Z^t} } = \sum_{r=0}^\infty{\DistRho_m(r) \gf{B}_r(Z) } \nonumber \\
    &= \gf{L}(Z) \sum_{r=0}^\infty{\DistRho_m(r) \gf{D}(Z)^r}    \nonumber 
		= \gf{L}(Z)\  \gf{R}_m (\gf{D}(Z)) \nonumber 
		\dominatedby \gf{\hat{L}}(Z)\  \gf{R}_\infty (\gf{D}(Z))  \nonumber \\
    &= \frac{(1-\beta)\,\gf{\hat{L}}(Z) }{1 - \beta \gf{D}(Z)}
		\, .
	\label{eq:gf-mu-relative}
	\end{align}
  The dominance notation above follows because
  $\gf{L} \dominatedby \gf{\hat{L}}$ and $\gf{R}_m \dominatedby \gf{R}_\infty$.


  For $\gf{B}(Z)$ to converge, we need to check that $\gf{D}(Z)$
  should never converge to $1/\beta$.  One can easily check that
  the radius of convergence of $\gf{D}(Z)$---which is
  $\displaystyle 1/\sqrt{1-\epsilon^2}$---is strictly less than $1/\beta$ when
  $\epsilon > 0$.  We conclude that $\gf{B}(Z)$ converges if
  both $\gf{D}(Z)$ and $\gf{L}(Z)$ converge.  The radius of
  convergence of $\gf{B}(Z)$ would be the smaller of the radii
  of convergence of $\gf{D}(Z)$ and $\gf{L}(Z)$.  We already
  know from the previous analysis that $\gf{\hat{L}}(Z)$ has the
  smaller radius of the two; therefore, the bound
  in~\eqref{eq:prob_forkable_gf} applies to the relative margin $\mu_x(y)$
  for $|x|\geq 0$. 
  \hfill $\qed$  
  % $\qedhere$
  % $\qed$
% \end{proof}


%=======================================================
\section{Proof of Bound~\ref{bound:geometric}}\label{sec:martingale-proof-new}

Let $\epsilon \in (0, 1)$,  
$W \in \{0,1\}^m, W^\prime \in \{0,1\}^k$ 
where both $(W_1, \ldots, W_n)$ and $(W^\prime_1, \ldots, W^\prime_n)$ 
satisfy the $\epsilon$-martingale condition. 
Let $B \in \{0,1\}^m, B^\prime \in \{0,1\}^k$ where the components of $B, B^\prime$ 
are independent with expectation $(1 - \epsilon)/2$.
By Lemma~\ref{lemma:rho-stationary}, 

\begin{equation*}\label{eq:WB-dominance}\tag{$*$}
  W \dominatedby B\quad \text{and} \quad W^\prime \dominatedby B^\prime
  \,. 
\end{equation*}

Let us define the partial order $\leq$ on Boolean strings $\{0,1\}^k, k \in \NN$ 
as follows: 
$a \leq b$ if and only if
for all $i \in [k]$, $a_i = 1$ implies $b_i = 1$. 
Let $\mu : \{0,1\}^k \rightarrow \Z$ be the margin function 
from Lemma~\ref{lem:relative-margin}. 
Observe that for Boolean strings $a, a^\prime, b, b^\prime$ 
with $|a| = |a^\prime|$ and $|b| = |b^\prime|$, 
(i.) $b \leq b^\prime$ implies $\mu_a(b) \leq \mu_a(b^\prime)$ and 
(ii.) $a \leq a^\prime$ implies $\mu_a(b) \leq \mu_{a^\prime}(b)$. 
That is, 
\begin{equation*}\label{eq:mu-WB-dominance}\tag{$\dagger$}
  \text{$\mu_a(b)$ is non-decreasing in both $a$ and $b$}
  \,.   
\end{equation*}

Using~\eqref{eq:WB-dominance} and~\eqref{eq:mu-WB-dominance}, 
it follows that $\mu_W(W^\prime) \dominatedby \mu_{B}(B^\prime)$. 
% Recall that $P \dominatedby Q$ is true if, 
% for all monotone sets $E$, 
% by $\Pr[ P \in E ] \leq \Pr[Q \in E]$. 
% In particular, concerning the dominance $\mu_W(W^\prime) \dominatedby \mu_{B}(B^\prime)$, 
Writing $x = W$ and $y = W^\prime$, we have 
\begin{align*}
  \Pr[\mu_x(y) \geq 0]\, 
    = \Pr[\mu_W(W^\prime) \geq 0]\, 
    \leq \Pr[\mu_{B}(B^\prime) \geq 0]
\end{align*}
where the inequality comes from the definition of stochastic dominance. 
A bound on the right-hand side 
is obtained in Bound~\ref{bound:analytic}. 
\hfill $\qed$






In \Section~\ref{sec:martingale-proof}, 
we present a weaker bound 
on $\Pr[\mu_x(y) \geq 0]$ where the sequence 
$x_1, \ldots, x_m, y_1, \ldots, y_k$ satisfies $\epsilon$-martingale conditions. 
The proof directly uses the properties of the martingale 
and Azuma's inequality  but 
it does not use a stochastic dominance argument. 
Although it gives a bound of $3 \exp\left( -\epsilon^4 (1 - O(\epsilon) ) k/64 \right)$, 
the reader might find the proof of independent interest. 


%
% The following lemma shows that $W$ is, in fact, 
% dominated by $B = (B_1, \ldots, B_k)$ where 
% each $B_i$ is an independent Bernoulli random variable 
% with parameter $(1 - \epsilon)/2$.
%
%--------------------------------
% \begin{lemma}\label{lem:dominance}
%   Let $X = (X_1, \ldots, X_n)$ be a family of random variables taking
%   values in $\{0,1\}$ with the property that, for each $i > 0$,
%   $\Exp[X_i \mid X_1, \ldots, X_{i-1}] \leq p$. Let
%   $B = (B_1, \ldots, B_n)$ be a family of independent random
%   variables, taking values in $\{0,1\}$, for which
%   $\Exp[B_i = 1] = p$.  Then $X \dominatedby B$.
% \end{lemma}
%
% \begin{proof}
%   We proceed by induction. The statement is clear for $n=1$. In
%   general, consider a random variable $X$ satisfying the conditions of
%   the theorem and taking values in $\{0,1\}^{n+1}$; let
%   $E \subset \{0,1\}^{n+1}$ be a monotone event. We wish to prove that
%   $\Pr[X \in E] \leq \Pr[B \in E]$.
%
%   We write $X = (Y, Z)$, where $Y$ takes values in $\{0,1\}^n$ and $Z$
%   in $\{0,1\}$. By induction, we may assume that
%   $Y \dominatedby (B_1, \ldots, B_n)$. Consider the events
%   \[
%     E_0 = \{ (y_1, \ldots, y_n) \mid (y_1, \ldots, y_n, 0) \in
%     E\}\qquad\text{and}\qquad E_1 = \{ (y_1, \ldots, y_n) \mid (y_1,
%     \ldots, y_n, 1) \in E\}\,;
%   \]
%   observe that the monotonicity of $E$ implies that
%   $E_0 \subseteq E_1$ and that $E_0, E_1$ are monotone. 
%   To study
%   $\Pr[X \in E]$, for an element
%   $y = (y_1, \ldots, y_n) \in \{0,1\}^n$ define
%   \[
%     q(y) = \Pr[ X \in E \mid Y = y]\,.
%   \]
%   Observe that $\Pr[X \in E] = \Exp[q(Y)]$ and, recalling that
%   $E_0 \subset E_1$, that
%   \begin{align*}
%     y \in E_0 & \Rightarrow q(y) = 1,\\
%     y \in E_1 \setminus E_0 & \Rightarrow q(y) \leq p\quad\text{by assumption, and}\\
%     y \not\in E_1 & \Rightarrow q(y) = 0\,.
%   \end{align*}
%   We conclude that
%   \begin{align}
%     \Pr[X \in E] & \leq \Pr[Y \in E_0] + p\cdot
%                    \Pr[Y \in E_1 \setminus E_0]
%                    = p \Pr[Y \in E_1] + (1-p) \Pr[Y \in E_0] \nonumber\\
%                  & \leq p \Pr[(B_1, \ldots, B_n) \in E_1] + (1-p) \Pr[(B_1, \ldots, B_n) \in E_0] \label{eq:XBdominance}\\
%                  &= \Pr[(B_1, \ldots, B_n) \in E_0] + p \Pr[(B_1, \ldots, B_n) \in E_1 \setminus E_0] = \Pr[B \in E]\,, \nonumber
%   \end{align}
%   as desired. The inequality of line~\eqref{eq:XBdominance} follows by
%   the induction hypothesis.
% \end{proof}
% ---------------------------------




\section{Proof of main theorems}\label{sec:thm-proofs}

\Paragraph{Proof of Theorem~\ref{thm:main}.}

Let us start with the following observation.
It allows us to formulate the
$(s, k)$-settlement insecurity of a distribution $\Distribution$
directly in terms of the relative margin.

\begin{lemma}\label{lemma:settlement-margin}
  Let $s, k, T \in \NN$. 
  Let $\Distribution$ be any distribution on $\{0,1\}^T$. 
  Then
  \[
    \mathbf{S}^{s,k}[\Distribution] \leq
      \Pr_{w \sim \Distribution} \left[\parbox{65mm}{
          there is a decomposition $w = x y z$, 
          where $|x| = s - 1$ and $|y| \geq k + 1$, 
          so that $\mu_x(y) \geq 0$
      }\right]
    \,.
  \]
\end{lemma}
\begin{proof}
  Lemma~\ref{lem:main-forks} implies that 
  $\mathbf{S}^{s,k}[\Distribution]$ is no more than 
  the probability that slot $s$ is not $k$-settled 
  for the characteristic string $w$. 
  By Observation~\ref{obs:settlement-balanced-fork}, 
  this probability, in turn, is no more than 
  the probability that there exists an $x$-balanced fork 
  $F \Fork xy$
  where we write $w = xyz, |x| = s - 1, |y| \geq k + 1, |z| \geq 0$. 
  Finally, Fact~\ref{fact:margin-balance} states that 
  for any characteristic string $xy$, 
  the two events ``exists an $x$-balanced fork $F \Fork xy$'' 
  and ``$\mu_x(y)$ is non-negative'' have the same measure. 
  Hence the claim follows. 
\end{proof}

If the distribution $\mathcal{D}$ in the lemma above 
satisfies the $\epsilon$-martingale condition, 
the probability in this lemma is no more than the probability 
in the left-hand side of Corollary~\ref{cor:main}. 
Finally, by retracing the proof of Corollary~\ref{cor:main} 
using the explicit probability from Bound~\ref{bound:geometric}, 
we see that the bound in Corollary~\ref{cor:main} is 
$O(1) \cdot \exp\bigl(-\Omega(\epsilon^3 (1 - O(\epsilon))k)\bigr)$. 
Since $\mathcal{B}_\epsilon$ satisfies the $\epsilon$-martingale condition, 
we conclude that $\mathbf{S}^{s,k}[\mathcal{B}_\epsilon]$ is no more than 
this quantity as well.
% \[
%   \mathbf{S}^{s,k}[\mathcal{B}_\epsilon] 
%       \leq O(1) \cdot \exp\bigl(-\Omega(\epsilon^3 (1 - O(\epsilon))k)\bigr)
%     \,.
% \]


For any player playing the settlement game, 
the set of strings on which the player wins is monotone 
with respect to the partial order $\leq$ defined in Section~\ref{sec:martingale-proof-new}. 
To see why, note that if the adversary wins with a specific string $w$, 
he can certainly win with any string $w^\prime$ where $w \leq w^\prime$. 
As $\mathcal{B}_\epsilon$ stochastically dominates $\mathcal{W}$, it follows that 
$
  \mathbf{S}^{s,k}[\mathcal{W}] \leq \mathbf{S}^{s,k}[\mathcal{B}_\epsilon]
$.

\hfill$\qed$


\Paragraph{Proof of Theorem~\ref{thm:main-CP}}
For the first inequality, observe that if $w$ violates $\kCP$, it must violate $\kSlotCP$ as well. 
It remains to prove the second inequality. 
Let $\Distribution$ be any distribution on $\{0,1\}^T$. 
We can apply Fact~\ref{fact:margin-balance} on the statement of Theorem~\ref{thm:cp-fork} 
to deduce that 
\begin{equation*}\label{eq:main-argument-1}
  \Pr_{w \sim \Distribution}[\text{$w$ violates $\kSlotCP$}] 
    \leq 
% \Pr_{\substack{w = x y z \\ |y| \geq k + 1} }[\text{exists a $k$-settlement violation} ] 
%    = 
    \Pr_{w \sim \Distribution}\left[\parbox{55mm}{
      there is a decomposition $w = xyz$, 
      where $|y| \geq k$, 
      so that $\mu_x(y) \geq 0$ 
    } \right] 
    \,.
\end{equation*}
By using a union bound over $|x|$, the above probability is at most 
\[
    \sum_{s = 1}^{T - k + 1} 
    \quad 
      \Pr_w\left[\parbox{60mm}{
        there is a decomposition $w = xyz$, 
        where $|x| = s - 1$ and $|y| \geq k$, 
        so that $\mu_x(y) \geq 0$ 
      }\right] 
    \,.
\]
Since $w$ satisfies the $\epsilon$-martingale condition, 
we can upper bound the probability inside the sum 
using Corollary~\ref{cor:main}. 
As we have seen in the proof of Theorem~\ref{thm:main}, 
the bound in Corollary~\ref{cor:main} is 
$
  O(1) \cdot \exp\bigl(-\Omega(\epsilon^3 (1 - O(\epsilon))k)\bigr)
  % \,.
$.
It follows that the sum above is at most $T \exp\bigl(-\Omega(\epsilon^3 (1 - O(\epsilon))k)\bigr)$.
\hfill $\qed$


It remains to prove the recursive formulation of the relative margin 
from \Section~\ref{sec:recursion}; 
we tackle it in the next section.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
